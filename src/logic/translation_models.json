{
  "gemini": {
    "name": "Google Gemini",
    "base_url": "https://generativelanguage.googleapis.com/v1beta/models",
    "models": {
      "gemini-2.0-flash": {
        "name": "Gemini Flash 2.0",
        "endpoint": "gemini-2.0-flash:generateContent",
        "temperature": 0.6
      },
      "gemini-2.5-flash-lite": {
        "name": "Gemini Flash 2.5 Lite",
        "endpoint": "gemini-2.5-flash-lite-preview-06-17:generateContent",
        "temperature": 0.6
      }
    }
  },
  "together": {
    "name": "Together AI",
    "base_url": "https://api.together.xyz/v1/chat/completions",
    "models": {
      "llama3.3-70b": {
        "name": "Llama 3.3 70B",
        "model_id": "meta-llama/Llama-3.3-70B-Instruct-Turbo",
        "max_tokens": 100000,
        "temperature": 0.6
      }
    }
  },
  "deepinfra": {
    "name": "DeepInfra",
    "base_url": "https://api.deepinfra.com/v1/openai/chat/completions",
    "models": {
      "llama-sao10k": {
        "name": "Sao10K L3.3-70B-Euryale",
        "model_id": "Sao10K/L3.3-70B-Euryale-v2.3",
        "max_tokens": 100000,
        "temperature": 0.6
      }
    }
  },
  "openai": {
    "name": "OpenAI",
    "base_url": "https://api.openai.com/v1/chat/completions",
    "models": {
      "gpt-4.1-nano": {
        "name": "GPT-4.1 Nano",
        "model_id": "gpt-4.1-nano",
        "max_tokens": 32000,
        "temperature": 0.6
      }
    }
  },
  "hyperbolic": {
    "name": "Hyperbolic",
    "base_url": "https://api.hyperbolic.xyz/v1/chat/completions",
    "models": {
      "llama-3.3-70b": {
        "name": "Llama 3.3 70B",
        "model_id": "meta-llama/Llama-3.3-70B-Instruct",
        "max_tokens": 100000,
        "temperature": 0.6
      }
    }
  },
  "chutes": {
    "name": "Chutes AI",
    "base_url": "https://llm.chutes.ai/v1/chat/completions",
    "models": {
      "mistral-3.2": {
        "name": "Mistral Small 3.2",
        "endpoint": "chutesai/Mistral-Small-3.2-24B-Instruct-2506",
        "max_tokens": 40000,
        "temperature": 0.15
      },
      "dolphin-mistral": {
        "name": "Dolphin Mistral",
        "endpoint": "cognitivecomputations/Dolphin3.0-Mistral-24B",
        "max_tokens": 40000,
        "temperature": 0.6
      },
      "skyfall": {
        "name": "Skyfall V2",
        "endpoint": "chutesai/Mistral-Small-3.2-24B-Instruct-2506",
        "max_tokens": 40000,
        "temperature": 0.3
      }
    }
  }
}
